'''
import streamlit as st
import json
import requests
from dotenv import load_dotenv
import os
from streamlit_chat import message
import time

# Load environment variables
load_dotenv()

API_KEY = os.getenv("API_KEY")
API_BASE = os.getenv("API_BASE")
API_KEY_2 = os.getenv("API_KEY_2")
API_BASE_2 = os.getenv("API_BASE_2")
MODEL_AGGREGATE = os.getenv("MODEL_AGGREGATE")
MODEL_REFERENCE_1 = os.getenv("MODEL_REFERENCE_1")
MODEL_REFERENCE_2 = os.getenv("MODEL_REFERENCE_2")
MODEL_REFERENCE_3 = os.getenv("MODEL_REFERENCE_3")

# Constants
MAX_TOKENS = int(os.getenv("MAX_TOKENS", 2048))
TEMPERATURE = float(os.getenv("TEMPERATURE", 0.7))
ROUNDS = int(os.getenv("ROUNDS", 1))

def generate_with_references(model, messages, references, temperature, max_tokens, api_base, api_key):
    # Implement the logic from the original code
    system_message = f"""You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.

Responses from models:
"""
    for i, reference in enumerate(references):
        system_message += f"\n{i+1}. {reference}"

    messages = [{"role": "system", "content": system_message}] + messages

    response = requests.post(
        f"{api_base}",
        json={
            "model": model,
            "messages": messages,
            "temperature": temperature if temperature > 1e-4 else 0,
            "max_tokens": max_tokens,
            "stream": True
        },
        headers={
            "accept": "application/json",
            "content-type": "application/json",
            "authorization": f"Bearer {api_key}"
        },
        stream=True
    )
    return response

def process_chunk(chunk):
    if 'choices' in chunk and chunk['choices']:
        for choice in chunk['choices']:
            if 'delta' in choice and 'content' in choice['delta']:
                return choice['delta']['content']
    return ""

def main():
    st.set_page_config(page_title="MoA: å¢å¼ºå¤§è¯­è¨€æ¨¡å‹", page_icon="ğŸ¶", layout="wide")
    
    # ä¿®æ”¹: æ›´æ–°CSSæ ·å¼ï¼Œç¡®ä¿è¾“å…¥æ¡†å›ºå®šåœ¨åº•éƒ¨ï¼Œå¹¶ä¸ºä¸»è¦å†…å®¹æ·»åŠ è¶³å¤Ÿçš„åº•éƒ¨å¡«å……
    st.markdown("""
    <style>
    .main {
        padding-bottom: 120px;  # å¢åŠ åº•éƒ¨å¡«å……
    }
    .fixed-bottom {
        position: fixed;
        bottom: 0;
        left: 0;
        right: 0;
        padding: 20px;
        background-color: white;
        z-index: 1000;
        box-shadow: 0 -2px 5px rgba(0,0,0,0.1);
    }
    .input-container {
        display: flex;
        align-items: center;
    }
    .stTextInput {
        flex-grow: 1;
    }
    .stButton {
        margin-left: 10px;
    }
    # æ·»åŠ ä»¥ä¸‹æ ·å¼æ¥ç¡®ä¿èŠå¤©å†…å®¹ä¸è¢«è¾“å…¥æ¡†é®æŒ¡
    .chat-content {
        margin-bottom: 100px;
    }
    </style>
    """, unsafe_allow_html=True)
    
    st.title("MoA: å¢å¼ºå¤§è¯­è¨€æ¨¡å‹")
    
    chat_column, status_column = st.columns([3, 1])
    
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "message_keys" not in st.session_state:
        st.session_state.message_keys = []
    
    with chat_column:
        # ä¿®æ”¹: æ·»åŠ ä¸€ä¸ªå®¹å™¨æ¥åŒ…è£¹èŠå¤©å†…å®¹ï¼Œå¹¶åº”ç”¨æ–°çš„CSSç±»
        chat_content = st.container()
        with chat_content:
            st.markdown('<div class="chat-content">', unsafe_allow_html=True)
            # æ˜¾ç¤ºèŠå¤©å†å²ï¼Œä½¿ç”¨ä¿å­˜çš„keys
            for i, (msg, key) in enumerate(zip(st.session_state.messages, st.session_state.message_keys)):
                if msg['role'] == 'user':
                    message(msg['content'], is_user=True, key=key)
                else:
                    message(msg['content'], is_user=False, key=key, avatar_style="bottts")
            st.markdown('</div>', unsafe_allow_html=True)
    
    with status_column:
        st.subheader("ç”ŸæˆçŠ¶æ€")
        status_placeholder = st.empty()
    
    # æ‚¬æµ®çš„è¾“å…¥æ¡†å’Œå‘é€æŒ‰é’®
    with st.container():
        st.markdown('<div class="fixed-bottom">', unsafe_allow_html=True)
        with st.form(key='my_form', clear_on_submit=True):
            cols = st.columns([4, 1])
            with cols[0]:
                user_input = st.text_input("What is your question?", key="user_input")
            with cols[1]:
                submit_button = st.form_submit_button("Send")
        st.markdown('</div>', unsafe_allow_html=True)
    
    if submit_button and user_input:
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°èŠå¤©å†å²
        user_msg = {"role": "user", "content": user_input}
        st.session_state.messages.append(user_msg)
        user_key = f"user_{len(st.session_state.messages)}_{int(time.time() * 1000)}"
        st.session_state.message_keys.append(user_key)
        
        # æ˜¾ç¤ºæ–°çš„ç”¨æˆ·æ¶ˆæ¯
        with chat_content:
            message(user_input, is_user=True, key=user_key)
        
        # ç”Ÿæˆå‚è€ƒå“åº”å’Œæœ€ç»ˆå“åº”çš„é€»è¾‘ä¿æŒä¸å˜
        reference_models = [MODEL_REFERENCE_1, MODEL_REFERENCE_2, MODEL_REFERENCE_3]
        references = []
        
        for i, model in enumerate(reference_models):
            status_placeholder.text(f"æ­£åœ¨å¤„ç†: {model}")
            time.sleep(1)  # å®é™…ä½¿ç”¨æ—¶å¯ä»¥åˆ é™¤è¿™è¡Œ
            response = generate_with_references(
                model=model,
                messages=[{"role": "user", "content": user_input}],
                references=[],
                temperature=TEMPERATURE,
                max_tokens=MAX_TOKENS,
                api_base=API_BASE,
                api_key=API_KEY
            )
            reference_output = ""
            for line in response.iter_lines():
                if line:
                    line_decoded = line.decode('utf-8').strip()
                    if line_decoded.startswith('data: {'):
                        json_data = line_decoded[6:]
                        try:
                            chunk = json.loads(json_data)
                            reference_output += process_chunk(chunk)
                        except json.JSONDecodeError:
                            pass
            references.append(reference_output)
        
        status_placeholder.text(f"æ­£åœ¨å¤„ç†: {MODEL_AGGREGATE}")
        final_response = generate_with_references(
            model=MODEL_AGGREGATE,
            messages=[{"role": "user", "content": user_input}],
            references=references,
            temperature=TEMPERATURE,
            max_tokens=MAX_TOKENS,
            api_base=API_BASE_2,
            api_key=API_KEY_2
        )
        
        full_response = ""
        for line in final_response.iter_lines():
            if line:
                line_decoded = line.decode('utf-8').strip()
                if line_decoded.startswith('data: {'):
                    json_data = line_decoded[6:]
                    try:
                        chunk = json.loads(json_data)
                        content = process_chunk(chunk)
                        full_response += content
                    except json.JSONDecodeError:
                        pass
        
        # æ·»åŠ AIå“åº”åˆ°èŠå¤©å†å²
        ai_msg = {"role": "assistant", "content": full_response}
        st.session_state.messages.append(ai_msg)
        ai_key = f"assistant_{len(st.session_state.messages)}_{int(time.time() * 1000)}"
        st.session_state.message_keys.append(ai_key)
        
        # æ˜¾ç¤ºæ–°çš„AIå“åº”
        with chat_content:
            message(full_response, is_user=False, key=ai_key, avatar_style="bottts")
        
        status_placeholder.text("ç”Ÿæˆå®Œæˆ")

if __name__ == "__main__":
    main()
'''
import streamlit as st
import json
import requests
from dotenv import load_dotenv
import os
from streamlit_chat import message
import time

# Load environment variables
load_dotenv()

API_KEY = os.getenv("API_KEY")
API_BASE = os.getenv("API_BASE")
API_KEY_2 = os.getenv("API_KEY_2")
API_BASE_2 = os.getenv("API_BASE_2")
MODEL_AGGREGATE = os.getenv("MODEL_AGGREGATE")
MODEL_REFERENCE_1 = os.getenv("MODEL_REFERENCE_1")
MODEL_REFERENCE_2 = os.getenv("MODEL_REFERENCE_2")
MODEL_REFERENCE_3 = os.getenv("MODEL_REFERENCE_3")

# Constants
MAX_TOKENS = int(os.getenv("MAX_TOKENS", 2048))
TEMPERATURE = float(os.getenv("TEMPERATURE", 0.7))
ROUNDS = int(os.getenv("ROUNDS", 1))

@st.cache(hash_funcs={requests.Response: lambda _: None}, show_spinner=False)
def generate_with_references(model, messages, references, temperature, max_tokens, api_base, api_key):
    system_message = f"""You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.
    You are the Mixture of Agents(MoA), which enhances Large Language Model Capabilities"""
    for i, reference in enumerate(references):
        system_message += f"\n{i+1}. {reference}"
    messages = [{"role": "system", "content": system_message}] + messages
    response = requests.post(
        f"{api_base}",
        json={
            "model": model,
            "messages": messages,
            "temperature": temperature if temperature > 1e-4 else 0,
            "max_tokens": max_tokens,
            "stream": True
        },
        headers={
            "accept": "application/json",
            "content-type": "application/json",
            "authorization": f"Bearer {api_key}"
        },
        stream=True
    )
    return response

@st.cache(show_spinner=False)
def process_chunk(chunk):
    if 'choices' in chunk and chunk['choices']:
        for choice in chunk['choices']:
            if 'delta' in choice and 'content' in choice['delta']:
                return choice['delta']['content']
    return ""

def main():
    st.set_page_config(page_title="MoA: å¢å¼ºå¤§è¯­è¨€æ¨¡å‹", page_icon="ğŸ¶", layout="wide")
    st.markdown("""
    <style>
    .main {
        padding-bottom: 120px;
    }
    .fixed-bottom {
        position: fixed;
        bottom: 0;
        left: 0;
        right: 0;
        padding: 20px;
        background-color: white;
        z-index: 1000;
        box-shadow: 0 -2px 5px rgba(0,0,0,0.1);
    }
    .input-container {
        display: flex;
        align-items: center;
    }
    .stTextInput {
        flex-grow: 1;
    }
    .stButton {
        margin-left: 10px;
    }
    .chat-content {
        margin-bottom: 100px;
    }
    </style>
    """, unsafe_allow_html=True)

    st.title("MoA: å¢å¼ºå¤§è¯­è¨€æ¨¡å‹")
    
    # åˆå§‹åŒ– session_state
    if 'messages' not in st.session_state:
        st.session_state['messages'] = []
    if 'message_keys' not in st.session_state:
        st.session_state['message_keys'] = []

    chat_column, status_column = st.columns([3, 1])
    with chat_column:
        chat_content = st.container()
        with chat_content:
            st.markdown('<div class="chat-content">', unsafe_allow_html=True)
            for i, (msg, key) in enumerate(zip(st.session_state.messages, st.session_state.message_keys)):
                if msg['role'] == 'user':
                    message(msg['content'], is_user=True, key=key)
                else:
                    message(msg['content'], is_user=False, key=key, avatar_style="bottts")
            st.markdown('</div>', unsafe_allow_html=True)
    
    with status_column:
        st.subheader("ç”ŸæˆçŠ¶æ€")
        status_placeholder = st.empty()
    
    with st.container():
        st.markdown('<div class="fixed-bottom">', unsafe_allow_html=True)
        with st.form(key='my_form', clear_on_submit=True):
            cols = st.columns([4, 1])
            with cols[0]:
                user_input = st.text_input("What is your question?", key="user_input")
            with cols[1]:
                submit_button = st.form_submit_button("Send")
        st.markdown('</div>', unsafe_allow_html=True)
    
    if submit_button and user_input:
        user_msg = {"role": "user", "content": user_input}
        st.session_state.messages.append(user_msg)
        user_key = f"user_{len(st.session_state.messages)}_{int(time.time() * 1000)}"
        st.session_state.message_keys.append(user_key)
        
        with chat_content:
            message(user_input, is_user=True, key=user_key)
        
        reference_models = [MODEL_REFERENCE_1, MODEL_REFERENCE_2, MODEL_REFERENCE_3]
        references = []
        for i, model in enumerate(reference_models):
            status_placeholder.text(f"æ­£åœ¨å¤„ç†: {model}")
            response = generate_with_references(
                model=model,
                messages=[{"role": "user", "content": user_input}],
                references=[],
                temperature=TEMPERATURE,
                max_tokens=MAX_TOKENS,
                api_base=API_BASE,
                api_key=API_KEY
            )
            reference_output = ""
            for line in response.iter_lines():
                if line:
                    line_decoded = line.decode('utf-8').strip()
                    if line_decoded.startswith('data: {'):
                        json_data = line_decoded[6:]
                        try:
                            chunk = json.loads(json_data)
                            reference_output += process_chunk(chunk)
                        except json.JSONDecodeError:
                            pass
            references.append(reference_output)
        
        status_placeholder.text(f"æ­£åœ¨å¤„ç†: {MODEL_AGGREGATE}")
        final_response = generate_with_references(
            model=MODEL_AGGREGATE,
            messages=[{"role": "user", "content": user_input}],
            references=references,
            temperature=TEMPERATURE,
            max_tokens=MAX_TOKENS,
            api_base=API_BASE_2,
            api_key=API_KEY_2
        )
        
        full_response = ""
        for line in final_response.iter_lines():
            if line:
                line_decoded = line.decode('utf-8').strip()
                if line_decoded.startswith('data: {'):
                    json_data = line_decoded[6:]
                    try:
                        chunk = json.loads(json_data)
                        content = process_chunk(chunk)
                        full_response += content
                    except json.JSONDecodeError:
                        pass
        
        ai_msg = {"role": "assistant", "content": full_response}
        st.session_state.messages.append(ai_msg)
        ai_key = f"assistant_{len(st.session_state.messages)}_{int(time.time() * 1000)}"
        st.session_state.message_keys.append(ai_key)
        
        with chat_content:
            message(full_response, is_user=False, key=ai_key, avatar_style="bottts")
        
        status_placeholder.text("ç”Ÿæˆå®Œæˆ")

if __name__ == "__main__":
    main()

